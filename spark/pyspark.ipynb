{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL ABOUT SPARK  TUTORIAL\n",
    "\n",
    "\n",
    "It is described as the general engine for large-scale data processing. It is used to analyze big data (enormous data) on local machines and also in clusters of computers using programming languages such as Python, Scala, and Java on cloud service providers such as Amazon Web Services (AWS Elastic MapReduce) and Azure (Databricks).\n",
    "\n",
    "  ### INSTALLING APACHE SPARK IN PC\n",
    "Spark runs on top of Scala, and Scala runs on the Java runtime environment. So, we will need to first install Java on our machine. Keep in mind that with Java, we will need to install it in a new folder that we can create on the local disk if we are using Windows.\n",
    "\n",
    "We will also need to install a python environment such as Anaconda.\n",
    "  \n",
    "We will need to configure the paths to these environments for Spark to run. We can run Spark in two main ways that we will be discussing. First, we can save the file that we are running as a .py file, or we can use a notebook such as VS Code where we will create an extension such as a .ipynb file (This will be our approach in this Spark tutorial). You can still create a Jupyter notebook, and it will work just fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW SPARK WORKS\n",
    "Before Spark 2.0, the main programming interface of Spark was the Resilient Distributed Dataset (RDD). After Spark 2.0, the RDD was replaced by the Dataset, which is similar to RDD but with more optimizations. Dataset has higher performance than RDD.     \n",
    "  ##### so how does rdd work? resilient distributed dataframe \n",
    " Let's imagine you have a large dataset that you want to analyze or process. Instead of trying to handle all that data on one computer, which might be slow or even impossible, you decide to spread it across multiple computers to speed things up.\n",
    "\n",
    "Here's where the Resilient Distributed Dataset (RDD) comes into play. Think of RDD as a way to chop up your big dataset into smaller pieces and distribute those pieces across a bunch of computers, forming a kind of team of processors.\n",
    "\n",
    "Now, RDD isn't just about spreading data around; it's also about making sure your data is safe. Let's say one of those computers fails or encounters an issue while processing its part of the data. RDD has a way to recover from that failure without losing the whole analysis. It's like having a backup plan in case one of your teammates needs a break.\n",
    "\n",
    "So, in simple terms, RDD helps you efficiently handle and process large amounts of data by breaking it into manageable pieces, distributing those pieces to different computers, and ensuring that your analysis can withstand hiccups without losing all your progress. It's like teamwork for big data!\n",
    "\n",
    "\n",
    "##### Dataset\n",
    "In the dataset(A Dataset is a distributed collection of data) we use the sparksql. A dataframe is a dataset organized into named columns and can be used in a relational database.Datasets were built on top of rdds ,they combine RDDs and dataframes and they provide perfomance benefits of the spark optimizer\n",
    "\n",
    "    BENEFITS OF DATASET\n",
    "   \n",
    "    - Ease of use\n",
    "   \n",
    "    - Optimization benefits  its fast\n",
    "   \n",
    "    - its sipports bothe batch and real time processing unlike the rdds which only support batch processing\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats new in spark 3.0\n",
    "\n",
    "1. It's faster - utilizes processes such as adaptive execution and dynamic partition pruning.\n",
    "\n",
    "2. There is deep Kubernetes support.\n",
    "\n",
    "3. There is binary file support.\n",
    "\n",
    "4. There is depreciation of the MLlib library (for RDD). (As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.)\n",
    "\n",
    "5. Takes advantage so that we can use GPU hardware.\n",
    "\n",
    "6. Makes use of Spark GraphX for analyzing social networks and how people are connected.\n",
    "\n",
    "7. There is ACID support for data lakes with Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanatages of using the dataset api rather than the rdd.\n",
    "1.Its scalable (can be used on many machines)\n",
    "2.its fast\n",
    "3.its hot (its common everywhere)\n",
    "4its easier to code in python,java and scala\n",
    "5.sparks runs everywhere- runs in hadoop,kubernetes,standaloneand in the cloud\n",
    "\n",
    "###### components of the spark core\n",
    " 1.mllib\n",
    "\n",
    " 2.spark streaming\n",
    " \n",
    " 3.sparksql\n",
    " \n",
    " 4.Graphx\n",
    "  ### Why are people moving to the sparksql(dataframe)\n",
    "   -contains row objects\n",
    "\n",
    "   -can run sql queries\n",
    " \n",
    "   -can have a schema\n",
    " \n",
    "  -can read and write to json\n",
    "\n",
    "   -also communcates with a jbdc connection\n",
    "\n",
    "   -can use a different mapper(line) to map a header to a dataset\n",
    "\n",
    "   -To use sql we infer the schema and register the dataframe as a table\n",
    "\n",
    "   - we use functions to avoid using rdds\n",
    "\n",
    "   #### why use python ?\n",
    "   1. There is no need to compile and manage dependencies\n",
    "   2. There is less coding overhead\n",
    "   3. Chances are  you alrady know python\n",
    "   #### Benefits of using it with scala\n",
    "   1. Scala is a more popular choice than spark\n",
    "   2. Spark is built in scala, so coding in scala is 'native ' to spark\n",
    "   3. New features and libraries tend to be scala first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is how our tuitorial will look like\n",
    " 1.We will learn how to use pyspark from the basics\n",
    "\n",
    " 2.we will explore how to use rdds and write some queries\n",
    " \n",
    " 3.we will move on to spark sql\n",
    " \n",
    " 4 we will go to basics od mllib and progress to more complex mllib such as tunning algorithms\n",
    " \n",
    " 5 we will then learn on spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be looking at spark sql and basics how to work eith datasets and ways of querying data using sparksql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### After fully configuring the spark (java,python,and hadoop)it should run succesfully\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will be rading datasets using the normal pandas  and we wil see how different it is from reading dataframe using spark\n",
    "import pandas as pd\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simon</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brenda</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sani</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mark</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>john</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name   age\n",
       "0   simon    34\n",
       "1  brenda    12\n",
       "2    sani    23\n",
       "3    mark    45\n",
       "4    john    79"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### We will be looking at the first rows of our data (we have just created this dataset for some simple manipulations however when we delve into deeper part of our project we will venture into more complex datasets)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We will need to first import spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We will also need to create the spark session and give it a name and remember after every spark session we will need to terminante it.\n",
    "spark=SparkSession.builder.appName('data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-57O8GOJ.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x205048e97d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we  will be reading the dataframe using pyspark using the code below (HOWEVER THIS IS NOT THE IDEAL WAY SINCE WE CAN SEE THAT EVEN THE HEADERS HAS BEEN MADE AS PART OF OUR DATAFRAME)\n",
    "df_spark1 = spark.read.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|   _c0| _c1|\n",
      "+------+----+\n",
      "|  name| age|\n",
      "| simon|  34|\n",
      "|brenda|  12|\n",
      "|  sani|  23|\n",
      "|  mark|  45|\n",
      "|  john|  79|\n",
      "|   sam|  67|\n",
      "| bruce|  33|\n",
      "| henry|  20|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## we will need to look at the schema but  we cannot read it effectively since we have not loaded our data correctly\n",
    "df_spark1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is the ideal way to read dataframes in pyspark where we set the header as true this makes sure that the header columns are not part of our dataset and the schema is well recognized \n",
    "### such that pyspark recognizes the correct datatypes in each column \n",
    "df_sparks= spark.read.csv('data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name| age|\n",
      "+------+----+\n",
      "| simon|  34|\n",
      "|brenda|  12|\n",
      "|  sani|  23|\n",
      "|  mark|  45|\n",
      "|  john|  79|\n",
      "|   sam|  67|\n",
      "| bruce|  33|\n",
      "| henry|  20|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sparks.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |--  age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Now we can see that it has correctly recognized datatypes such as integer before it regarded all datatypes as strings\n",
    "df_sparks.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_sparks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', ' age']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### when we want to identify the columns of our data\n",
    "df_sparks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "| simon|\n",
      "|brenda|\n",
      "|  sani|\n",
      "|  mark|\n",
      "|  john|\n",
      "|   sam|\n",
      "| bruce|\n",
      "| henry|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We want only to get data from our 'name' column\n",
    "df_sparks.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), (' age', 'int')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### we are looking at the datatypes of our data\n",
    "df_sparks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+\n",
      "|summary|  name|               age|\n",
      "+-------+------+------------------+\n",
      "|  count|     8|                 8|\n",
      "|   mean|  NULL|            39.125|\n",
      "| stddev|  NULL|23.381540337869712|\n",
      "|    min|brenda|                12|\n",
      "|    max| simon|                79|\n",
      "+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### similar to pandas pysaprk also can do statistical analysis of our data using the .describe function\n",
    "df_sparks.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+--------+\n",
      "|  name| age|age_rank|\n",
      "+------+----+--------+\n",
      "|  john|  79|       1|\n",
      "|   sam|  67|       2|\n",
      "|  mark|  45|       3|\n",
      "| simon|  34|       4|\n",
      "| bruce|  33|       5|\n",
      "|  sani|  23|       6|\n",
      "| henry|  20|       7|\n",
      "|brenda|  12|       8|\n",
      "+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## These will be complex a little bit where we will be ranking the ages and we will add another column showing the rank of these\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Assuming df_sparks is your DataFrame\n",
    "windowSpec = Window.orderBy(col(' age').desc())\n",
    "\n",
    "df_sparks = df_sparks.withColumn('age_rank', rank().over(windowSpec))\n",
    "\n",
    "df_sparks.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name| age|\n",
      "+------+----+\n",
      "| simon|  34|\n",
      "|brenda|  12|\n",
      "|  sani|  23|\n",
      "|  mark|  45|\n",
      "|  john|  79|\n",
      "|   sam|  67|\n",
      "| bruce|  33|\n",
      "| henry|  20|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##We will be dropping a column\n",
    "df_sparks.drop(\"Experience after 2 years\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  Name| age|\n",
      "+------+----+\n",
      "| simon|  34|\n",
      "|brenda|  12|\n",
      "|  sani|  23|\n",
      "|  mark|  45|\n",
      "|  john|  79|\n",
      "|   sam|  67|\n",
      "| bruce|  33|\n",
      "| henry|  20|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We will see how we can rename our 'name' column to 'Name'\n",
    "df_sparks.withColumnRenamed('name','Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will load another dataframe and we will do some manipulation on these dataframe( Hope we still learning)\n",
    "###  We will be working with handling missing values and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We will start another sesssion\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remember we wil need to initiate another sparksession \n",
    "spark=SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-57O8GOJ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cd025dfd10>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember this is the ideal way of loading data when learning pyspark\n",
    "df_sparks1= spark.read.csv('missing.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "|simon|  34|         2|  NULL|\n",
      "| mark|  45|        22| 34000|\n",
      "|  ivy|NULL|         1|  2000|\n",
      "|  eva|  50|        34| 80000|\n",
      "|  eve|  78|        35|  NULL|\n",
      "|brian|  55|        23| 56000|\n",
      "| josh|  66|      NULL| 78000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Remember we have loaded another dataset that has null values\n",
    "df_sparks1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "| mark| 45|        22| 34000|\n",
      "|  eva| 50|        34| 80000|\n",
      "|brian| 55|        23| 56000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be dropping the null values and this approach is not advocated for since its drops  the whole row even when missing only one value\n",
    "df_sparks1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "|simon|  34|         2|  NULL|\n",
      "| mark|  45|        22| 34000|\n",
      "|  ivy|NULL|         1|  2000|\n",
      "|  eva|  50|        34| 80000|\n",
      "|  eve|  78|        35|  NULL|\n",
      "|brian|  55|        23| 56000|\n",
      "| josh|  66|      NULL| 78000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be introducing another function where we add 'all' .This only drops a whole row if all the values in the colum are null\n",
    "df_sparks1.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "|simon|  34|         2|  NULL|\n",
      "| mark|  45|        22| 34000|\n",
      "|  ivy|NULL|         1|  2000|\n",
      "|  eva|  50|        34| 80000|\n",
      "|  eve|  78|        35|  NULL|\n",
      "|brian|  55|        23| 56000|\n",
      "| josh|  66|      NULL| 78000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we can add another input to our function thats a threshold where we will be dropping a row when all the null values exceed a certain number\n",
    "df_sparks1.na.drop(how='all',thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+\n",
      "| name| age|experience|salary|\n",
      "+-----+----+----------+------+\n",
      "| mark|  45|        22| 34000|\n",
      "|  ivy|NULL|         1|  2000|\n",
      "|  eva|  50|        34| 80000|\n",
      "|brian|  55|        23| 56000|\n",
      "| josh|  66|      NULL| 78000|\n",
      "+-----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we further can add input to our function when we are narrowing down to how we are dropping the columns (in the below code we are narrowing down to the column 'salary')\n",
    "df_sparks1.na.drop(how='all',subset=['salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will be filling the missing (null)values using the mean or median.Mostly median is advocated for to reduce the effect of skeweness of our data.We will be filling our missing values using the median and the mean  respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "| name| age|experience|salary|age_imputed|experience_imputed|salary_imputed|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "|simon|  34|         2|  NULL|         34|                 2|         50000|\n",
      "| mark|  45|        22| 34000|         45|                22|         34000|\n",
      "|  ivy|NULL|         1|  2000|         54|                 1|          2000|\n",
      "|  eva|  50|        34| 80000|         50|                34|         80000|\n",
      "|  eve|  78|        35|  NULL|         78|                35|         50000|\n",
      "|brian|  55|        23| 56000|         55|                23|         56000|\n",
      "| josh|  66|      NULL| 78000|         66|                19|         78000|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_sparks1).transform(df_sparks1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###We have filled now using the median\n",
    "from pyspark.ml.feature import Imputer\n",
    "imputer =Imputer(\n",
    "    inputCols=['age','experience','salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c)for c in ['age','experience','salary']]\n",
    ").setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "| name| age|experience|salary|age_imputed|experience_imputed|salary_imputed|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "|simon|  34|         2|  NULL|         34|                 2|         56000|\n",
      "| mark|  45|        22| 34000|         45|                22|         34000|\n",
      "|  ivy|NULL|         1|  2000|         50|                 1|          2000|\n",
      "|  eva|  50|        34| 80000|         50|                34|         80000|\n",
      "|  eve|  78|        35|  NULL|         78|                35|         56000|\n",
      "|brian|  55|        23| 56000|         55|                23|         56000|\n",
      "| josh|  66|      NULL| 78000|         66|                22|         78000|\n",
      "+-----+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## now we have the dataframe combined with the new data (the dataframe where all the missing values are filled with the median values)\n",
    "imputer.fit(df_sparks1).transform(df_sparks1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FILTER OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|name| age|experience|salary|\n",
      "+----+----+----------+------+\n",
      "|mark|  45|        22| 34000|\n",
      "| ivy|NULL|         1|  2000|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##3 we will be filtering the rows where the salary is less or equal to 50000\n",
    "df_sparks1.filter('salary<=50000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|name| age|\n",
      "+----+----+\n",
      "|mark|  45|\n",
      "| ivy|NULL|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we will be only selecting the two columns where the data meets the condition we have above\n",
    "df_sparks1.filter('salary<=50000').select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| name|age|experience|salary|\n",
      "+-----+---+----------+------+\n",
      "| mark| 45|        22| 34000|\n",
      "|  eva| 50|        34| 80000|\n",
      "|brian| 55|        23| 56000|\n",
      "| josh| 66|      NULL| 78000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be  filering the data where salary is between 20000 and 80000\n",
    "df_sparks1.filter((df_sparks1['salary']<=80000)& (df_sparks1['salary']>=20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| mark| 45|\n",
      "|  eva| 50|\n",
      "|brian| 55|\n",
      "| josh| 66|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be selecting only two columns whose criteria meets the threshold we have above\n",
    "df_sparks1.filter((df_sparks1['salary']<=80000)& (df_sparks1['salary']>=20000)).select(['name','age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.stop of <pyspark.sql.session.SparkSession object at 0x000001CD025DFD10>>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### We will have to stop the spark session and we will have to terminate it\n",
    "spark.stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY AND FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "###since we are starting another spark session we will have to terminate our previous session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We willl have to build a sparksession\n",
    "sparks=SparkSession.builder.appName('dept').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-57O8GOJ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cd025dfd10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we will load our data in spark\n",
    "df_spark2= spark.read.csv('department.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+\n",
      "| name|   dept|salary|\n",
      "+-----+-------+------+\n",
      "|simon|   data| 20000|\n",
      "|simon|bigdata| 40000|\n",
      "|simon|     ux| 30000|\n",
      "| mark|   data| 45000|\n",
      "|  ben|     ux| 56000|\n",
      "|  tom|bigdata| 22000|\n",
      "|  ben|     ux| 34000|\n",
      "|  tom|   data| 12000|\n",
      "| mark|     ux|  1000|\n",
      "+-----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be looking at our datatypes and also this helps in looking if there are nulls in our data\n",
    "df_spark2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|sum(salary)|\n",
      "+-----+-----------+\n",
      "|simon|      90000|\n",
      "|  ben|      90000|\n",
      "| mark|      46000|\n",
      "|  tom|      34000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### As we have seen from the data above we will be looking the sum of salry where we will be grouping by names\n",
    "df_spark2.groupBy('name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|   dept|sum(salary)|\n",
      "+-------+-----------+\n",
      "|   data|      77000|\n",
      "|bigdata|      62000|\n",
      "|     ux|     121000|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we will be grouping the data using the dept column \n",
    "df_spark2.groupBy('dept').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|avg(salary)|\n",
      "+-----+-----------+\n",
      "|simon|    30000.0|\n",
      "|  ben|    45000.0|\n",
      "| mark|    23000.0|\n",
      "|  tom|    17000.0|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we will be looking for the mean where we will be grouping by name\n",
    "df_spark2.groupBy('name').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   dept|count|\n",
      "+-------+-----+\n",
      "|   data|    3|\n",
      "|bigdata|    2|\n",
      "|     ux|    4|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### We will be counting the number of items by group using the dept column\n",
    "df_spark2.groupBy('dept').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|     260000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### we will be aggregating all the salary of all  the members in our department\n",
    "df_spark2.agg({'salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESILIENT DISTRIBUTED DATASET\n",
    "\n",
    "Altough depreciating rdds cannot be ignored in matters relating to spark .Its still supported and its still useful.We will be manipulating data using both rdd and sparksql and we will be seeing the differenece in the execution of coe and we will definitely see the easier approach of the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kindly remember we will be comparing all the queries we have learnt using rdd to the dataframe using sparksql below and see how different they are from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 325.3333333333333)\n",
      "(26, 242.05882352941177)\n",
      "(55, 295.53846153846155)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(27, 228.125)\n",
      "(53, 222.85714285714286)\n",
      "(57, 258.8333333333333)\n",
      "(56, 306.6666666666667)\n",
      "(43, 230.57142857142858)\n",
      "(36, 246.6)\n",
      "(22, 206.42857142857142)\n",
      "(35, 211.625)\n",
      "(45, 309.53846153846155)\n",
      "(60, 202.71428571428572)\n",
      "(67, 214.625)\n",
      "(19, 213.27272727272728)\n",
      "(30, 235.8181818181818)\n",
      "(51, 302.14285714285717)\n",
      "(25, 197.45454545454547)\n",
      "(21, 350.875)\n",
      "(42, 303.5)\n",
      "(49, 184.66666666666666)\n",
      "(48, 281.4)\n",
      "(50, 254.6)\n",
      "(39, 169.28571428571428)\n",
      "(32, 207.9090909090909)\n",
      "(58, 116.54545454545455)\n",
      "(64, 281.3333333333333)\n",
      "(31, 267.25)\n",
      "(52, 340.6363636363636)\n",
      "(24, 233.8)\n",
      "(20, 165.0)\n",
      "(62, 220.76923076923077)\n",
      "(41, 268.55555555555554)\n",
      "(44, 282.1666666666667)\n",
      "(69, 235.2)\n",
      "(65, 298.2)\n",
      "(61, 256.22222222222223)\n",
      "(28, 209.1)\n",
      "(66, 276.44444444444446)\n",
      "(46, 223.69230769230768)\n",
      "(29, 215.91666666666666)\n",
      "(18, 343.375)\n",
      "(47, 233.22222222222223)\n",
      "(34, 245.5)\n",
      "(63, 384.0)\n",
      "(23, 246.3)\n"
     ]
    }
   ],
   "source": [
    "#We will be implementing a script that finds the average number of friends by age  in our 'fake friends'.csv \n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf=SparkConf().setMaster('local').setAppName(\"friends\")\n",
    "sc= SparkContext(conf=conf)\n",
    "## in this function when you look at our data we have choosen our third and fourth  column  and we cast them as interger values so we can perfom arithmetic calculations on them look at the data\n",
    "# and see that we have ignored our first and second column\n",
    "def parseLine(line):\n",
    "    fields=line.split(',')\n",
    "    age=int(fields[2])\n",
    "    numFriends=int(fields[3])\n",
    "    return(age,numFriends)\n",
    "## we load our data\n",
    "lines =sc.textFile('fakefriends.csv')\n",
    "\n",
    "## now we have a new rdd that contains the age and the number of friends in our dataset\n",
    "\n",
    "rdd=lines.map(parseLine)\n",
    "\n",
    "# In the first lambda it ensures that our first column(age column) remains untouched [map values leaves the key untouched  (in our data the key is the age column)] and  the values (numfriends)\n",
    "#are put into our function\n",
    "#The second lambda  aggregates everything together for each age(new rdd)\n",
    "totals=rdd.mapValues(lambda x: (x,1)).reduceByKey(lambda x, y: (x[0]+y[0], x[1]+ y[1]))\n",
    "\n",
    "## Total number of friends and total number of times that age was encountered and divide  and we  get the final result\n",
    "averages=totals.mapValues(lambda x: x[0] /x[1])\n",
    "results=averages.collect()\n",
    "for result in results:\n",
    "    print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTERING IN RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "## In the follwoing  code we will be running to find the minimum weather observed in the year 1800 we will be using the 1800.csv\n",
    "\n",
    "# importing the dependencies\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"MinTemperatures\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "# our function is splitting each lines by coommas extacting the station is from first field and entry type  and extracting the temperature from the thirs field and\n",
    "# change it to float and convert it to F\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n",
    "\n",
    "lines = sc.textFile(\"1800.csv\")\n",
    "parsedLines = lines.map(parseLine)\n",
    "\n",
    "# we will be looking at the min if its not min the it does not get passed on\n",
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1])\n",
    "stationTemps = minTemps.map(lambda x: (x[0], x[2]))\n",
    "\n",
    "# we combine all the minimum values for each rdd and find the min\n",
    "minTemps = stationTemps.reduceByKey(lambda x, y: min(x,y))\n",
    "results = minTemps.collect();\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3309.38, 45)\n",
      "(3790.570000000001, 79)\n",
      "(3924.230000000001, 96)\n",
      "(4042.6499999999987, 23)\n",
      "(4172.289999999998, 99)\n",
      "(4178.500000000001, 75)\n",
      "(4278.049999999997, 36)\n",
      "(4297.260000000001, 98)\n",
      "(4316.299999999999, 47)\n",
      "(4327.729999999999, 77)\n",
      "(4367.62, 13)\n",
      "(4384.33, 48)\n",
      "(4394.599999999999, 49)\n",
      "(4475.569999999999, 94)\n",
      "(4505.79, 67)\n",
      "(4517.27, 50)\n",
      "(4524.509999999999, 78)\n",
      "(4561.069999999999, 5)\n",
      "(4628.4, 57)\n",
      "(4635.799999999997, 83)\n",
      "(4642.259999999999, 91)\n",
      "(4647.129999999999, 74)\n",
      "(4652.939999999999, 84)\n",
      "(4659.63, 3)\n",
      "(4664.589999999998, 12)\n",
      "(4681.919999999999, 66)\n",
      "(4701.019999999999, 56)\n",
      "(4707.41, 21)\n",
      "(4727.860000000001, 80)\n",
      "(4735.030000000001, 14)\n",
      "(4735.200000000002, 37)\n",
      "(4755.070000000001, 7)\n",
      "(4756.8899999999985, 44)\n",
      "(4765.05, 31)\n",
      "(4812.489999999998, 82)\n",
      "(4815.050000000002, 4)\n",
      "(4819.700000000001, 10)\n",
      "(4830.549999999999, 88)\n",
      "(4836.859999999999, 20)\n",
      "(4851.479999999999, 89)\n",
      "(4876.840000000002, 95)\n",
      "(4898.460000000002, 38)\n",
      "(4904.209999999999, 76)\n",
      "(4908.81, 86)\n",
      "(4915.889999999999, 27)\n",
      "(4921.27, 18)\n",
      "(4945.299999999999, 53)\n",
      "(4958.600000000001, 1)\n",
      "(4975.22, 51)\n",
      "(4979.06, 16)\n",
      "(4990.72, 30)\n",
      "(5000.709999999998, 28)\n",
      "(5019.449999999999, 22)\n",
      "(5032.529999999999, 29)\n",
      "(5032.679999999999, 17)\n",
      "(5040.709999999999, 60)\n",
      "(5057.610000000001, 25)\n",
      "(5059.4299999999985, 19)\n",
      "(5112.709999999999, 81)\n",
      "(5123.010000000001, 69)\n",
      "(5140.3499999999985, 65)\n",
      "(5152.290000000002, 11)\n",
      "(5155.419999999999, 35)\n",
      "(5186.429999999999, 40)\n",
      "(5206.4, 87)\n",
      "(5245.059999999999, 52)\n",
      "(5250.4, 26)\n",
      "(5253.3200000000015, 62)\n",
      "(5254.659999999998, 33)\n",
      "(5259.920000000003, 24)\n",
      "(5265.750000000001, 93)\n",
      "(5288.689999999996, 64)\n",
      "(5290.409999999998, 90)\n",
      "(5298.090000000002, 55)\n",
      "(5322.649999999999, 9)\n",
      "(5330.8, 34)\n",
      "(5337.44, 72)\n",
      "(5368.249999999999, 70)\n",
      "(5368.83, 43)\n",
      "(5379.280000000002, 92)\n",
      "(5397.879999999998, 6)\n",
      "(5413.510000000001, 15)\n",
      "(5415.150000000001, 63)\n",
      "(5437.7300000000005, 58)\n",
      "(5496.050000000004, 32)\n",
      "(5497.479999999998, 61)\n",
      "(5503.43, 85)\n",
      "(5517.240000000001, 8)\n",
      "(5524.949999999998, 0)\n",
      "(5637.62, 41)\n",
      "(5642.89, 59)\n",
      "(5696.840000000003, 42)\n",
      "(5963.109999999999, 46)\n",
      "(5977.189999999995, 97)\n",
      "(5994.59, 2)\n",
      "(5995.660000000003, 71)\n",
      "(6065.389999999999, 54)\n",
      "(6193.109999999999, 39)\n",
      "(6206.199999999999, 73)\n",
      "(6375.449999999997, 68)\n"
     ]
    }
   ],
   "source": [
    "### In the following rdd  its adds up the toal amount spent by each customer .\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"SpendByCustomerSorted\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "#The customer id becomes our key and the values becomes the amounnt spent\n",
    "def extractCustomerPricePairs(line):\n",
    "    fields = line.split(',')\n",
    "    return (int(fields[0]), float(fields[2]))\n",
    "\n",
    "input = sc.textFile(\"customer-orders.csv\")\n",
    "mappedInput = input.map(extractCustomerPricePairs)\n",
    "\n",
    " ## adds all the vlaues encountered for all our customer id \n",
    "totalByCustomer = mappedInput.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "#Changed for Python 3 compatibility  :the below two lines of code just sort the values from  the least to the largest value\n",
    "#flipped = totalByCustomer.map(lambda (x,y):(y,x))\n",
    "flipped = totalByCustomer.map(lambda x: (x[1], x[0]))\n",
    "\n",
    "totalByCustomerSorted = flipped.sortByKey()\n",
    "\n",
    "results = totalByCustomerSorted.collect();\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we have seen the implementation of  rdds in code ,Its the original way of using spark and as we can see its helpful to learn as it still widely used in manipulation of data using spark we will then comapare the code we have seen so far while using rdd to using saprk sql .awe will be comparing which one is easier implementing the code above and also the run time for the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparing the rdd to sparksql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|age|average_friends|\n",
      "+---+---------------+\n",
      "| 18|         343.38|\n",
      "| 19|         213.27|\n",
      "| 20|          165.0|\n",
      "| 21|         350.88|\n",
      "| 22|         206.43|\n",
      "| 23|          246.3|\n",
      "| 24|          233.8|\n",
      "| 25|         197.45|\n",
      "| 26|         242.06|\n",
      "| 27|         228.13|\n",
      "| 28|          209.1|\n",
      "| 29|         215.92|\n",
      "| 30|         235.82|\n",
      "| 31|         267.25|\n",
      "| 32|         207.91|\n",
      "| 33|         325.33|\n",
      "| 34|          245.5|\n",
      "| 35|         211.63|\n",
      "| 36|          246.6|\n",
      "| 37|         249.33|\n",
      "+---+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In the first code we were finding the number of friends by age and we will compare that code while using sparksql(uses the  dataframe api)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as functions\n",
    "\n",
    "spark =SparkSession.builder.appName('friends').getOrCreate()\n",
    "\n",
    "### we will load our data in spark\n",
    "lines= spark.read.csv('fakefriends-header.csv',header=True,inferSchema=True)\n",
    "\n",
    "# Select the columns \"age\" and \"friends\"\n",
    "friendsbyage = lines.select(\"age\", \"friends\")\n",
    "\n",
    "# Group by 'age' and calculate the average number of friends, rounding to 2 decimal places\n",
    "average_friends_by_age = friendsbyage.groupBy('age').agg(round(avg('friends'), 2).alias('average_friends'))\n",
    "\n",
    "# Order the DataFrame by 'age'\n",
    "sorted_average_friends = average_friends_by_age.orderBy('age')\n",
    "\n",
    "# Show the result\n",
    "sorted_average_friends.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stationID: string (nullable = true)\n",
      " |-- date: integer (nullable = true)\n",
      " |-- measure_type: string (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      "\n",
      "+-----------+----------------+\n",
      "|  stationID|min(temperature)|\n",
      "+-----------+----------------+\n",
      "|ITE00100554|          -148.0|\n",
      "|EZE00100082|          -135.0|\n",
      "+-----------+----------------+\n",
      "\n",
      "ITE00100554\t5.36F\n",
      "EZE00100082\t7.70F\n"
     ]
    }
   ],
   "source": [
    "## In the following line fo code we will  be finding the min temperature  where we will create a schema to convert our data to a datframe\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MinTemperatures\").getOrCreate()\n",
    "\n",
    "schema = StructType([ \\\n",
    "                     StructField(\"stationID\", StringType(), True), \\\n",
    "                     StructField(\"date\", IntegerType(), True), \\\n",
    "                     StructField(\"measure_type\", StringType(), True), \\\n",
    "                     StructField(\"temperature\", FloatType(), True)])\n",
    "\n",
    "# // Read the file as dataframe\n",
    "df = spark.read.schema(schema).csv(\"1800.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "# Filter out all but TMIN entries\n",
    "minTemps = df.filter(df.measure_type == \"TMIN\")\n",
    "\n",
    "# Select only stationID and temperature\n",
    "stationTemps = minTemps.select(\"stationID\", \"temperature\")\n",
    "\n",
    "# Aggregate to find minimum temperature for every station\n",
    "minTempsByStation = stationTemps.groupBy(\"stationID\").min(\"temperature\")\n",
    "minTempsByStation.show()\n",
    "\n",
    "# Convert temperature to fahrenheit and sort the dataset\n",
    "minTempsByStationF = minTempsByStation.withColumn(\"temperature\",\n",
    "                                                  func.round(func.col(\"min(temperature)\") * 0.1 * (9.0 / 5.0) + 32.0, 2))\\\n",
    "                                                  .select(\"stationID\", \"temperature\").sort(\"temperature\")\n",
    "                                                  \n",
    "# Collect, format, and print the results\n",
    "results = minTempsByStationF.collect()\n",
    "\n",
    "for result in results:\n",
    "    print(result[0] + \"\\t{:.2f}F\".format(result[1]))\n",
    "    \n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|cust_id|total_spent|\n",
      "+-------+-----------+\n",
      "|     45|    3309.38|\n",
      "|     79|    3790.57|\n",
      "|     96|    3924.23|\n",
      "|     23|    4042.65|\n",
      "|     99|    4172.29|\n",
      "|     75|     4178.5|\n",
      "|     36|    4278.05|\n",
      "|     98|    4297.26|\n",
      "|     47|     4316.3|\n",
      "|     77|    4327.73|\n",
      "|     13|    4367.62|\n",
      "|     48|    4384.33|\n",
      "|     49|     4394.6|\n",
      "|     94|    4475.57|\n",
      "|     67|    4505.79|\n",
      "|     50|    4517.27|\n",
      "|     78|    4524.51|\n",
      "|      5|    4561.07|\n",
      "|     57|     4628.4|\n",
      "|     83|     4635.8|\n",
      "|     91|    4642.26|\n",
      "|     74|    4647.13|\n",
      "|     84|    4652.94|\n",
      "|      3|    4659.63|\n",
      "|     12|    4664.59|\n",
      "|     66|    4681.92|\n",
      "|     56|    4701.02|\n",
      "|     21|    4707.41|\n",
      "|     80|    4727.86|\n",
      "|     14|    4735.03|\n",
      "|     37|     4735.2|\n",
      "|      7|    4755.07|\n",
      "|     44|    4756.89|\n",
      "|     31|    4765.05|\n",
      "|     82|    4812.49|\n",
      "|      4|    4815.05|\n",
      "|     10|     4819.7|\n",
      "|     88|    4830.55|\n",
      "|     20|    4836.86|\n",
      "|     89|    4851.48|\n",
      "|     95|    4876.84|\n",
      "|     38|    4898.46|\n",
      "|     76|    4904.21|\n",
      "|     86|    4908.81|\n",
      "|     27|    4915.89|\n",
      "|     18|    4921.27|\n",
      "|     53|     4945.3|\n",
      "|      1|     4958.6|\n",
      "|     51|    4975.22|\n",
      "|     16|    4979.06|\n",
      "|     30|    4990.72|\n",
      "|     28|    5000.71|\n",
      "|     22|    5019.45|\n",
      "|     29|    5032.53|\n",
      "|     17|    5032.68|\n",
      "|     60|    5040.71|\n",
      "|     25|    5057.61|\n",
      "|     19|    5059.43|\n",
      "|     81|    5112.71|\n",
      "|     69|    5123.01|\n",
      "|     65|    5140.35|\n",
      "|     11|    5152.29|\n",
      "|     35|    5155.42|\n",
      "|     40|    5186.43|\n",
      "|     87|     5206.4|\n",
      "|     52|    5245.06|\n",
      "|     26|     5250.4|\n",
      "|     62|    5253.32|\n",
      "|     33|    5254.66|\n",
      "|     24|    5259.92|\n",
      "|     93|    5265.75|\n",
      "|     64|    5288.69|\n",
      "|     90|    5290.41|\n",
      "|     55|    5298.09|\n",
      "|      9|    5322.65|\n",
      "|     34|     5330.8|\n",
      "|     72|    5337.44|\n",
      "|     70|    5368.25|\n",
      "|     43|    5368.83|\n",
      "|     92|    5379.28|\n",
      "|      6|    5397.88|\n",
      "|     15|    5413.51|\n",
      "|     63|    5415.15|\n",
      "|     58|    5437.73|\n",
      "|     32|    5496.05|\n",
      "|     61|    5497.48|\n",
      "|     85|    5503.43|\n",
      "|      8|    5517.24|\n",
      "|      0|    5524.95|\n",
      "|     41|    5637.62|\n",
      "|     59|    5642.89|\n",
      "|     42|    5696.84|\n",
      "|     46|    5963.11|\n",
      "|     97|    5977.19|\n",
      "|      2|    5994.59|\n",
      "|     71|    5995.66|\n",
      "|     54|    6065.39|\n",
      "|     39|    6193.11|\n",
      "|     73|     6206.2|\n",
      "|     68|    6375.45|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## We will also be looking for the total amount spent by a customers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TotalSpentByCustomer\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Create schema when reading customer-orders\n",
    "customerOrderSchema = StructType([ \\\n",
    "                                  StructField(\"cust_id\", IntegerType(), True),\n",
    "                                  StructField(\"item_id\", IntegerType(), True),\n",
    "                                  StructField(\"amount_spent\", FloatType(), True)\n",
    "                                  ])\n",
    "\n",
    "# Load up the data into spark dataset\n",
    "customersDF = spark.read.schema(customerOrderSchema).csv(\"customer-orders.csv\")\n",
    "\n",
    "totalByCustomer = customersDF.groupBy(\"cust_id\").agg(func.round(func.sum(\"amount_spent\"), 2) \\\n",
    "                                      .alias(\"total_spent\"))\n",
    "\n",
    "totalByCustomerSorted = totalByCustomer.sort(\"total_spent\")\n",
    "\n",
    "totalByCustomerSorted.show(totalByCustomerSorted.count())\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have seen how the two codes compare and we can see the using the dataframe api is realtively easier of the two to code and  also the time spent in execution of the code is relatively  faster  and thus some of the reason why many developers and many people in the industry are going for the dataframe api .These are some of the codes that are been implemented  and examples below we willl be looking at  writing indepth pyspark queries for manipulation using BIG DATA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlib is Spark’s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. At a high level, it provides tools such as:\n",
    "\n",
    "1. ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "\n",
    "2. Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "\n",
    "3. Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "\n",
    "4. Persistence: saving and load algorithms, models, and Pipelines\n",
    "\n",
    "5. Utilities: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "\n",
    "As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n",
    "\n",
    "\n",
    "#### What are the implications?\n",
    "\n",
    "1. MLlib will still support the RDD-based API in spark.mllib with bug fixes.\n",
    "2. MLlib will not add new features to the RDD-based API.\n",
    "3. In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\n",
    "\n",
    "#### Why is MLlib switching to the DataFrame-based API?\n",
    "\n",
    "1. DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n",
    "2. The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n",
    "3. DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n",
    "   What is “Spark ML”?\n",
    "\n",
    "“Spark ML” is not an official name but occasionally used to refer to the MLlib DataFrame-based API. This is majorly due to the org.apache.spark.ml Scala package name used by the DataFrame-based API, and the “Spark ML Pipelines” term we used initially to emphasize the pipeline concept.\n",
    "\n",
    "#### Is MLlib deprecated?\n",
    "\n",
    "No. MLlib includes both the RDD-based API and the DataFrame-based API. The RDD-based API is now in maintenance mode. But neither API is deprecated, nor MLlib as a whole.\n",
    "\n",
    "These are some of the main points  that are worth noting as we shift  to using the dataframe api .You can refer to the documentation for further information\n",
    "\n",
    "#### Why Mllib in machine learning is becoming more used\n",
    "\n",
    "1. Ease of use -Usable in Java, Scala, Python, and R.\n",
    "2. Perfomance - High-quality algorithms, 100x faster than MapReduce.\n",
    "3. Runs everywhere- Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud, against diverse data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the code blocks  we will be implementing machine learning from basics and we scale towards bulding more accurate by also tuning the hyperparameters to achieve better accuracy.We will be using simple python scripts and scale towards complex queries making thie machine learning tuitorial begginner friendly as possible.  We will be perfoming a regression task and a classification task in this tuitorial .(Hope we still learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PYSAPRK REGRESSION MODEL ALGORHTHIM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In the following notebook we will be predicting the prices of houses in paris based on 1.SquareMetres 2. Number of rooms 3. Whether it has a yard or not 4.Whether has a pool or not 5. Number of floors 6.city code 7CityPartRange 8. Number of previous owners 9.is new built 10. Whether has a storm Protector or not 11. Whether has a basement or not 12. Whether has a garage 13. Whether has a storage room or not 14. Has a guest room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- squareMeters: integer (nullable = true)\n",
      " |-- numberOfRooms: integer (nullable = true)\n",
      " |-- hasYard: integer (nullable = true)\n",
      " |-- hasPool: integer (nullable = true)\n",
      " |-- floors: integer (nullable = true)\n",
      " |-- cityCode: integer (nullable = true)\n",
      " |-- cityPartRange: integer (nullable = true)\n",
      " |-- numPrevOwners: integer (nullable = true)\n",
      " |-- made: integer (nullable = true)\n",
      " |-- isNewBuilt: integer (nullable = true)\n",
      " |-- hasStormProtector: integer (nullable = true)\n",
      " |-- basement: integer (nullable = true)\n",
      " |-- attic: integer (nullable = true)\n",
      " |-- garage: integer (nullable = true)\n",
      " |-- hasStorageRoom: integer (nullable = true)\n",
      " |-- hasGuestRoom: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('housing').getOrCreate()\n",
    "house_df = spark.read.csv('ParisHousing.csv', header = True, inferSchema = True)\n",
    "house_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>squareMeters</th>\n",
       "      <th>numberOfRooms</th>\n",
       "      <th>hasYard</th>\n",
       "      <th>hasPool</th>\n",
       "      <th>floors</th>\n",
       "      <th>cityCode</th>\n",
       "      <th>cityPartRange</th>\n",
       "      <th>numPrevOwners</th>\n",
       "      <th>made</th>\n",
       "      <th>isNewBuilt</th>\n",
       "      <th>hasStormProtector</th>\n",
       "      <th>basement</th>\n",
       "      <th>attic</th>\n",
       "      <th>garage</th>\n",
       "      <th>hasStorageRoom</th>\n",
       "      <th>hasGuestRoom</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>49870.1312</td>\n",
       "      <td>50.3584</td>\n",
       "      <td>0.5087</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>50.2763</td>\n",
       "      <td>50225.4861</td>\n",
       "      <td>5.5101</td>\n",
       "      <td>5.5217</td>\n",
       "      <td>2005.4885</td>\n",
       "      <td>0.4991</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>5033.1039</td>\n",
       "      <td>5028.0106</td>\n",
       "      <td>553.1212</td>\n",
       "      <td>0.503</td>\n",
       "      <td>4.9946</td>\n",
       "      <td>4993447.525749963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>28774.37535029503</td>\n",
       "      <td>28.81669636927458</td>\n",
       "      <td>0.4999493023602426</td>\n",
       "      <td>0.5000147612582521</td>\n",
       "      <td>28.88917127111252</td>\n",
       "      <td>29006.675799293178</td>\n",
       "      <td>2.87202417160515</td>\n",
       "      <td>2.8566667927002753</td>\n",
       "      <td>9.308089589340009</td>\n",
       "      <td>0.5000241918339955</td>\n",
       "      <td>0.5000249918746555</td>\n",
       "      <td>2876.7295448116365</td>\n",
       "      <td>2894.3322098165813</td>\n",
       "      <td>262.0501698906411</td>\n",
       "      <td>0.5000160013441162</td>\n",
       "      <td>3.1764098913678978</td>\n",
       "      <td>2877424.109945015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10313.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>99999</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>99953</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.00067712E7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary       squareMeters      numberOfRooms             hasYard  \\\n",
       "0   count              10000              10000               10000   \n",
       "1    mean         49870.1312            50.3584              0.5087   \n",
       "2  stddev  28774.37535029503  28.81669636927458  0.4999493023602426   \n",
       "3     min                 89                  1                   0   \n",
       "4     max              99999                100                   1   \n",
       "\n",
       "              hasPool             floors            cityCode  \\\n",
       "0               10000              10000               10000   \n",
       "1              0.4968            50.2763          50225.4861   \n",
       "2  0.5000147612582521  28.88917127111252  29006.675799293178   \n",
       "3                   0                  1                   3   \n",
       "4                   1                100               99953   \n",
       "\n",
       "      cityPartRange       numPrevOwners               made  \\\n",
       "0             10000               10000              10000   \n",
       "1            5.5101              5.5217          2005.4885   \n",
       "2  2.87202417160515  2.8566667927002753  9.308089589340009   \n",
       "3                 1                   1               1990   \n",
       "4                10                  10               2021   \n",
       "\n",
       "           isNewBuilt   hasStormProtector            basement  \\\n",
       "0               10000               10000               10000   \n",
       "1              0.4991              0.4999           5033.1039   \n",
       "2  0.5000241918339955  0.5000249918746555  2876.7295448116365   \n",
       "3                   0                   0                   0   \n",
       "4                   1                   1               10000   \n",
       "\n",
       "                attic             garage      hasStorageRoom  \\\n",
       "0               10000              10000               10000   \n",
       "1           5028.0106           553.1212               0.503   \n",
       "2  2894.3322098165813  262.0501698906411  0.5000160013441162   \n",
       "3                   1                100                   0   \n",
       "4               10000               1000                   1   \n",
       "\n",
       "         hasGuestRoom              price  \n",
       "0               10000              10000  \n",
       "1              4.9946  4993447.525749963  \n",
       "2  3.1764098913678978  2877424.109945015  \n",
       "3                   0            10313.5  \n",
       "4                  10       1.00067712E7  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the below code we will be looking a the distribution of all the numerical columns in our dataset\n",
    "house_df.describe().toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to for price  squareMeters 0.9999993570640745\n",
      "Correlation to for price  numberOfRooms 0.009590905935479128\n",
      "Correlation to for price  hasYard -0.006119244882540526\n",
      "Correlation to for price  hasPool -0.00507034083386251\n",
      "Correlation to for price  floors 0.0016542562406504926\n",
      "Correlation to for price  cityCode -0.001539367348580816\n",
      "Correlation to for price  cityPartRange 0.008812911660535336\n",
      "Correlation to for price  numPrevOwners 0.016618826067943387\n",
      "Correlation to for price  made -0.007209526254690673\n",
      "Correlation to for price  isNewBuilt -0.010642774359518865\n",
      "Correlation to for price  hasStormProtector 0.0074959113342807394\n",
      "Correlation to for price  basement -0.003967482178851144\n",
      "Correlation to for price  attic -0.000599514077496332\n",
      "Correlation to for price  garage -0.017229051207338166\n",
      "Correlation to for price  hasStorageRoom -0.0034852993013792864\n",
      "Correlation to for price  hasGuestRoom -0.0006439241048174541\n",
      "Correlation to for price  price 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Here we will look at the correlation of our indipendent variables in respect to our target variable column\n",
    "\n",
    "import six\n",
    "for i in house_df.columns:\n",
    "    if not( isinstance(house_df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to for price \", i, house_df.stat.corr('price',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('squareMeters', 'int'),\n",
       " ('numberOfRooms', 'int'),\n",
       " ('hasYard', 'int'),\n",
       " ('hasPool', 'int'),\n",
       " ('floors', 'int'),\n",
       " ('cityCode', 'int'),\n",
       " ('cityPartRange', 'int'),\n",
       " ('numPrevOwners', 'int'),\n",
       " ('made', 'int'),\n",
       " ('isNewBuilt', 'int'),\n",
       " ('hasStormProtector', 'int'),\n",
       " ('basement', 'int'),\n",
       " ('attic', 'int'),\n",
       " ('garage', 'int'),\n",
       " ('hasStorageRoom', 'int'),\n",
       " ('hasGuestRoom', 'int'),\n",
       " ('price', 'double')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "house_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### missing values\n",
    "\n",
    "#### after looking  whether our data has null values ,remember the previous code that we have written in earlier stages of our tuitorial ,then we could have have used those methods to filll in the missing values.keep note that there is no defined way of handling missing values and the approach that we use mainly depend on the specific data we are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------+-------+------+--------+-------------+-------------+----+----------+-----------------+--------+-----+------+--------------+------------+-----+\n",
      "|squareMeters|numberOfRooms|hasYard|hasPool|floors|cityCode|cityPartRange|numPrevOwners|made|isNewBuilt|hasStormProtector|basement|attic|garage|hasStorageRoom|hasGuestRoom|price|\n",
      "+------------+-------------+-------+-------+------+--------+-------------+-------------+----+----------+-----------------+--------+-----+------+--------------+------------+-----+\n",
      "|           0|            0|      0|      0|     0|       0|            0|            0|   0|         0|                0|       0|    0|     0|             0|           0|    0|\n",
      "+------------+-------------+-------+-------+------+--------+-------------+-------------+----+----------+-----------------+--------+-----+------+--------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "null_counts = house_df.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in house_df.columns])\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model building\n",
    " ##### In our data as we can clearly see all the data is an integer format.Thus we will not be encoding our data,However for general knowledge we would have used the string indexer to encode our values so that our machine learning model can understand our data. However we have handles this in the classification tuitorial that we will be learning as we progress into later stages of  this tuitorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will be using a vector assemblor just as the name suggests it will be a vector where we combine all our indipendent column (x values if we were building ml models in pandas)into one long vector.The VectorAssembler is commonly used in Spark machine learning pipelines to prepare features for model training. By combining multiple input columns into a single vector column, it enables easier processing and compatibility with Spark ML algorithms that expect input in vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(squareMeters=75523, numberOfRooms=3, hasYard=0, hasPool=1, floors=63, cityCode=9373, cityPartRange=3, numPrevOwners=8, made=2005, isNewBuilt=0, hasStormProtector=1, basement=4313, attic=9005, garage=956, hasStorageRoom=0, hasGuestRoom=7, price=7559081.5, features=DenseVector([75523.0, 3.0, 0.0, 1.0, 63.0, 9373.0, 3.0, 8.0, 2005.0, 0.0, 1.0, 4313.0, 9005.0, 956.0, 0.0, 7.0]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = \n",
    "['squareMeters', 'numberOfRooms', 'hasYard', 'hasPool', 'floors', 'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'isNewBuilt', 'hasStormProtector', 'basement', 'attic', 'garage', 'hasStorageRoom', 'hasGuestRoom'\n",
    "], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(house_df)\n",
    "\n",
    "vhouse_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['squareMeters', 'numberOfRooms', 'hasYard', 'hasPool', 'floors', 'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'isNewBuilt', 'hasStormProtector', 'basement', 'attic', 'garage', 'hasStorageRoom', 'hasGuestRoom', 'price']\n"
     ]
    }
   ],
   "source": [
    "column_list = house_df.columns\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|    price|\n",
      "+--------------------+---------+\n",
      "|[75523.0,3.0,0.0,...|7559081.5|\n",
      "|[80771.0,39.0,1.0...|8085989.5|\n",
      "|[55712.0,58.0,0.0...|5574642.1|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vhouse_df = vhouse_df.select(['features', 'price'])\n",
    "vhouse_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be training our data into our train and test dataframe\n",
    "splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEAR REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [100.00021861637728,-0.5175076000027911,2980.046796688874,2980.973072158858,54.85448717034218,-0.0013110530884183462,47.626320875614155,-1.1101269980230821,-3.1465764010972412,108.76773471426917,130.1536880715282,0.0014356327042063847,-0.006889181027848809,0.11070896922565554,33.642134737808966,-4.465488608949495]\n",
      "Intercept: 6658.399968358891\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='price', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1900.416648\n",
      "r2: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('squareMeters', 'int'),\n",
       " ('numberOfRooms', 'int'),\n",
       " ('hasYard', 'int'),\n",
       " ('hasPool', 'int'),\n",
       " ('floors', 'int'),\n",
       " ('cityCode', 'int'),\n",
       " ('cityPartRange', 'int'),\n",
       " ('numPrevOwners', 'int'),\n",
       " ('made', 'int'),\n",
       " ('isNewBuilt', 'int'),\n",
       " ('hasStormProtector', 'int'),\n",
       " ('basement', 'int'),\n",
       " ('attic', 'int'),\n",
       " ('garage', 'int'),\n",
       " ('hasStorageRoom', 'int'),\n",
       " ('hasGuestRoom', 'int'),\n",
       " ('price', 'double')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(squareMeters=75523, numberOfRooms=3, hasYard=0, hasPool=1, floors=63, cityCode=9373, cityPartRange=3, numPrevOwners=8, made=2005, isNewBuilt=0, hasStormProtector=1, basement=4313, attic=9005, garage=956, hasStorageRoom=0, hasGuestRoom=7, price=7559081.5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  price|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|13183.681253584029|13229.1|[123.0,61.0,0.0,0...|\n",
      "|16647.408403486486|15488.0|[128.0,38.0,0.0,1...|\n",
      "|20891.090756537975|22670.7|[141.0,16.0,0.0,1...|\n",
      "|17874.423979471714|17071.0|[143.0,27.0,0.0,0...|\n",
      "|16317.835583469037|16799.2|[148.0,91.0,0.0,0...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"price\",\"features\").show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"price\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  price|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|13183.681253584029|13229.1|[123.0,61.0,0.0,0...|\n",
      "|16647.408403486486|15488.0|[128.0,38.0,0.0,1...|\n",
      "|20891.090756537975|22670.7|[141.0,16.0,0.0,1...|\n",
      "|17874.423979471714|17071.0|[143.0,27.0,0.0,0...|\n",
      "|16317.835583469037|16799.2|[148.0,91.0,0.0,0...|\n",
      "|23500.506945738438|22499.2|[153.0,71.0,0.0,1...|\n",
      "|18481.649008751654|17363.0|[163.0,6.0,0.0,0....|\n",
      "|27748.883387062084|26533.9|[187.0,96.0,1.0,1...|\n",
      "|28275.707007199715|28295.6|[202.0,18.0,1.0,0...|\n",
      "|24367.995726818004|24058.9|[211.0,10.0,0.0,0...|\n",
      "|26747.636314698957|27438.4|[229.0,49.0,1.0,0...|\n",
      "| 35917.63878875479|34373.4|[302.0,23.0,1.0,0...|\n",
      "| 38013.89273764825|37972.0|[350.0,65.0,0.0,0...|\n",
      "| 42207.31617907483|39316.3|[369.0,25.0,0.0,0...|\n",
      "| 46270.79047106901|45524.8|[371.0,29.0,1.0,0...|\n",
      "| 45420.40686515109|44847.8|[388.0,11.0,1.0,0...|\n",
      "|49139.835387514206|54382.3|[405.0,38.0,0.0,1...|\n",
      "| 47500.89740371859|47257.3|[460.0,3.0,0.0,0....|\n",
      "|47762.485317394254|47967.6|[460.0,62.0,0.0,0...|\n",
      "|53078.896006394556|52352.6|[464.0,16.0,0.0,1...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"price\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Our model is perfoming at its best very rare chances of this but the perfomance of the data on the test and train data is remarkable. The model is perfoming at at 100% .Means that we have tuned the hyperparameters properly but cannot also mean that our data is too perfect to perfom prediction on .This kind of data we cannot actualy find it in the industry however, if you ever perfom a linear regression task on a dataset using spark this is an ideal way to approach that problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECISION TREE REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 91431.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'price')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score on train data = 0.999027\n",
      "R2 score on test data = 0.998989\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Assuming you already have the required imports and have trained the DecisionTreeRegressor\n",
    "\n",
    "# Evaluate R2 score on train data\n",
    "train_predictions = dt_model.transform(train_df)\n",
    "train_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "train_r2 = train_evaluator.evaluate(train_predictions)\n",
    "print(\"R2 score on train data = %g\" % train_r2)\n",
    "\n",
    "# Evaluate R2 score on test data\n",
    "test_predictions = dt_model.transform(test_df)\n",
    "test_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "test_r2 = test_evaluator.evaluate(test_predictions)\n",
    "print(\"R2 score on test data = %g\" % test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(16, {0: 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|        prediction|  price|            features|\n",
      "+------------------+-------+--------------------+\n",
      "|127262.40527384522|13229.1|[123.0,61.0,0.0,0...|\n",
      "| 97906.28473723664|15488.0|[128.0,38.0,0.0,1...|\n",
      "|151952.45069790722|22670.7|[141.0,16.0,0.0,1...|\n",
      "|144904.70835000914|17071.0|[143.0,27.0,0.0,0...|\n",
      "|125369.92808927408|16799.2|[148.0,91.0,0.0,0...|\n",
      "|153383.51573638432|22499.2|[153.0,71.0,0.0,1...|\n",
      "|155024.78433768972|17363.0|[163.0,6.0,0.0,0....|\n",
      "| 156321.1876818048|26533.9|[187.0,96.0,1.0,1...|\n",
      "|176166.17372363282|28295.6|[202.0,18.0,1.0,0...|\n",
      "|155417.36407659348|24058.9|[211.0,10.0,0.0,0...|\n",
      "| 160866.8837550857|27438.4|[229.0,49.0,1.0,0...|\n",
      "|144001.76410902606|34373.4|[302.0,23.0,1.0,0...|\n",
      "|177090.16379258948|37972.0|[350.0,65.0,0.0,0...|\n",
      "|164157.72139291026|39316.3|[369.0,25.0,0.0,0...|\n",
      "|161809.16011527993|45524.8|[371.0,29.0,1.0,0...|\n",
      "|170860.33814884422|44847.8|[388.0,11.0,1.0,0...|\n",
      "|141449.04365600704|54382.3|[405.0,38.0,0.0,1...|\n",
      "|151631.49113271057|47257.3|[460.0,3.0,0.0,0....|\n",
      "| 148477.5401683421|47967.6|[460.0,62.0,0.0,0...|\n",
      "|156831.64327706656|52352.6|[464.0,16.0,0.0,1...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'price', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'price', 'features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we can see the r2 score of the decision tree regressor is also perfoming fairly well.We can see that althogh our model does not have hyperparameters  its still perfoming well (you can look at how the decision tree model  works on its offical documentation)\n",
    "\n",
    "\n",
    "#### That will be all to cover as matters to regression models techniques are concerned(hope we are still learning)Lets move on to our next type of supervised machine learning model- The classification model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following machine learning project we will be creating a project that predicts whether a customer willl commit to a bank deposit based on some certain idependent variables.This can be used to assess the credit risk of certain customers\n",
    "\n",
    "\n",
    "\n",
    "Input variables: age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome. taget variable :deposit(Yes/No)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('customer-churn').getOrCreate()\n",
    "df = spark.read.csv('bank.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
      "| 56|     admin.| married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
      "| 41| technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
      "| 55|   services| married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
      "| 54|     admin.| married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
      "| 42| management|  single| tertiary|     no|      0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|    yes|\n",
      "| 56| management| married| tertiary|     no|    830|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|    yes|\n",
      "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|    yes|\n",
      "| 37| technician| married|secondary|     no|      1|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|    yes|\n",
      "| 28|   services|  single|secondary|     no|   5090|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|    yes|\n",
      "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|    yes|\n",
      "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|    yes|\n",
      "| 29| management| married| tertiary|     no|    199|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|    yes|\n",
      "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|    yes|\n",
      "| 31| technician|  single| tertiary|     no|    703|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|    yes|\n",
      "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|    yes|\n",
      "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|    yes|\n",
      "| 49|   services| married|secondary|     no|     -8|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|    yes|\n",
      "| 41|     admin.| married|secondary|     no|     55|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|    yes|\n",
      "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|    yes|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the following column we willl be loooking whether our data has missing or null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|job|marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|  0|  0|      0|        0|      0|      0|      0|   0|      0|  0|    0|       0|       0|    0|       0|       0|      0|\n",
      "+---+---+-------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "null_counts = df.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  in the following we will be assessing the target column variable where we will see if our target column is balanced if not then we will need to balance the taget variable as our model would then be biased to a certain class.Keep in mind that this is a very important step in matters regarding building classification models are concerned and when the target column is imbalanced  means that our model becomes biased to a ceetain class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deposit</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>5873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>5289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  deposit  count\n",
       "0      no   5873\n",
       "1     yes   5289"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.groupby('deposit').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we will be looking at the summary statistics of our numeric columns . This comes in handy when dealing with outliers and also looking at the distribution of the various columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "      <td>11162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>41.231947679627304</td>\n",
       "      <td>1528.5385235620856</td>\n",
       "      <td>15.658036194230425</td>\n",
       "      <td>371.99381831213043</td>\n",
       "      <td>2.508421429851281</td>\n",
       "      <td>51.33040673714388</td>\n",
       "      <td>0.8325568894463358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>11.913369192215518</td>\n",
       "      <td>3225.413325946149</td>\n",
       "      <td>8.420739541006462</td>\n",
       "      <td>347.12838571630687</td>\n",
       "      <td>2.7220771816614824</td>\n",
       "      <td>108.75828197197717</td>\n",
       "      <td>2.292007218670508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>18</td>\n",
       "      <td>-6847</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>95</td>\n",
       "      <td>81204</td>\n",
       "      <td>31</td>\n",
       "      <td>3881</td>\n",
       "      <td>63</td>\n",
       "      <td>854</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 age             balance                 day  \\\n",
       "0   count               11162               11162               11162   \n",
       "1    mean  41.231947679627304  1528.5385235620856  15.658036194230425   \n",
       "2  stddev  11.913369192215518   3225.413325946149   8.420739541006462   \n",
       "3     min                  18               -6847                   1   \n",
       "4     max                  95               81204                  31   \n",
       "\n",
       "             duration            campaign               pdays  \\\n",
       "0               11162               11162               11162   \n",
       "1  371.99381831213043   2.508421429851281   51.33040673714388   \n",
       "2  347.12838571630687  2.7220771816614824  108.75828197197717   \n",
       "3                   2                   1                  -1   \n",
       "4                3881                  63                 854   \n",
       "\n",
       "             previous  \n",
       "0               11162  \n",
       "1  0.8325568894463358  \n",
       "2   2.292007218670508  \n",
       "3                   0  \n",
       "4                  58  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "df.select(numeric_features).describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
    "cols = df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We will start with category indexing ,One Hot encoding and Vector Assembler-a feature transformer that merges multiple columns into a vector column The code is available at the databricks site and it indexes each categorical column using the string Indexer and converts the indexed categories into one hot encoded varibales. The resulting output has the binary vectors appended to the end of each row. We use the StringIndexer again to encode our labels to label indices. Next, we use the VectorAssembler to combine all the feature columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Vector Assembler: A VectorAssembler is created to assemble all the feature columns, including the one-hot encoded categorical columns and the numeric columns, into a single feature vector named 'features'. The input columns are specified as assemblerInputs, which is a concatenation of the one-hot encoded categorical column names and the numeric column names. The VectorAssembler is added to the stages list.\n",
    "\n",
    "##### The stages list will contain all the necessary stages of the data preprocessing pipeline, including string indexing, one-hot encoding, label string indexing, and vector assembling. These stages can then be used in a Pipeline for further processing or training a machine learning mod\n",
    "\n",
    "##### we will then use a pipeline to chain multiple transformers and estimators together to specify our algorithm workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>deposit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2343</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1042</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>56</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>45</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1467</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>41</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1270</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1389</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>55</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2476</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>54</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>184</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>673</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           features  age         job  \\\n",
       "0    1.0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   59      admin.   \n",
       "1    1.0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   56      admin.   \n",
       "2    1.0  (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   41  technician   \n",
       "3    1.0  (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...   55    services   \n",
       "4    1.0  (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   54      admin.   \n",
       "\n",
       "   marital  education default  balance housing loan  contact  duration  \\\n",
       "0  married  secondary      no     2343     yes   no  unknown      1042   \n",
       "1  married  secondary      no       45      no   no  unknown      1467   \n",
       "2  married  secondary      no     1270     yes   no  unknown      1389   \n",
       "3  married  secondary      no     2476     yes   no  unknown       579   \n",
       "4  married   tertiary      no      184      no   no  unknown       673   \n",
       "\n",
       "   campaign  pdays  previous poutcome deposit  \n",
       "0         1     -1         0  unknown     yes  \n",
       "1         1     -1         0  unknown     yes  \n",
       "2         1     -1         0  unknown     yes  \n",
       "3         1     -1         0  unknown     yes  \n",
       "4         2     -1         0  unknown     yes  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7855\n",
      "Test Dataset Count: 3307\n"
     ]
    }
   ],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECISIONTREECLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision trees are widely used since they can easily interpret, handle categorical data, extend multiclass classifications, and capture non-linearities and feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "|age|       job|label| rawPrediction|prediction|         probability|\n",
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "| 33|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 49|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 52|management|  0.0|[520.0,1931.0]|       1.0|[0.21215830273357...|\n",
      "| 53|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 58|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 32|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 57|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 52|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 46|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "| 31|management|  0.0|[2498.0,481.0]|       0.0|[0.83853642161799...|\n",
      "+---+----------+-----+--------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.7808118726917547\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that our decision tree algorithm is perfom poorly .Howevwer we can improve its perfomance by using using ensemble methods such as a random Forest classifier .Uses multiple decision trees thus abetter perfomance is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RANDOMFORESTCLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "|age|       job|label|       rawPrediction|prediction|         probability|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "| 33|management|  0.0|[13.7531588468082...|       0.0|[0.68765794234041...|\n",
      "| 49|management|  0.0|[13.4127191903380...|       0.0|[0.67063595951690...|\n",
      "| 52|management|  0.0|[7.65584951235426...|       1.0|[0.38279247561771...|\n",
      "| 53|management|  0.0|[11.6803776655343...|       0.0|[0.58401888327671...|\n",
      "| 58|management|  0.0|[14.3894234094507...|       0.0|[0.71947117047253...|\n",
      "| 32|management|  0.0|[13.8880450105089...|       0.0|[0.69440225052544...|\n",
      "| 57|management|  0.0|[13.2319245839861...|       0.0|[0.66159622919930...|\n",
      "| 52|management|  0.0|[16.8463444127981...|       0.0|[0.84231722063990...|\n",
      "| 46|management|  0.0|[16.7636699072479...|       0.0|[0.83818349536239...|\n",
      "| 31|management|  0.0|[13.3442134015346...|       0.0|[0.66721067007673...|\n",
      "+---+----------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8789997993785139\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model is perfoming better when we use an ensemble model and the higher the area under ROC means a better perfomance and a better accuracy. That bring us to an end of this tuitorial in matters consaning mllib and machine learning matters.We have covered both classifcation and regresssion tasks at scale and now henceforth you can cover any machine learning model using spark .(Hope we still learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARKSTREAMING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Structured Streaming makes it easy to build streaming applications and pipelines with the same and familiar Spark APIs.\n",
    "\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive. You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides fast, scalable, fault-tolerant, end-to-end exactly-once stream processing without the user having to reason about streaming.\n",
    "\n",
    "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees. However, since Spark 2.3, we have introduced a new low-latency processing mode called Continuous Processing, which can achieve end-to-end latencies as low as 1 millisecond with at-least-once guarantees. Without changing the Dataset/DataFrame operations in your queries, you will be able to choose the mode based on your application requirements.\n",
    "\n",
    "\n",
    "#### Advantages of using spark for streaming\n",
    "\n",
    "1. Easy to use - Spark Structured Streaming abstracts away complex streaming concepts such as incremental processing, checkpointing, and watermarks so that you can build streaming applications and pipelines without learning any new concepts or tools.\n",
    "2. Unified batch and streaming APIs- Spark Structured Streaming provides the same structured APIs (DataFrames and Datasets) as Spark so that you don’t need to develop on or maintain two different technology stacks for batch and streaming. In addition, unified APIs make it easy to migrate your existing batch Spark jobs to streaming jobs.\n",
    "3. Low latency and cost effective - Spark Structured Streaming uses the same underlying architecture as Spark so that you can take advantage of all the performance and cost optimizations built into the Spark engine. With Spark Structured Streaming, you can build low latency streaming applications and pipelines cost effectively.\n",
    "\n",
    "For more information, you can refer to the documentation of Spark and learn more about Spark Streaming and its components in the following [link](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#quick-example).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hope we have learned alot from this tutorial,keep on practising and with time you will understand this spark library and stat implementing it in code.Remember we  need to understand this framework to help us understand big data technologies in depth and actually manipulate  big data .A common way of handling big data that is becoming quite frequent is actually doing this computation of big data using clusters provided by cloud technologies such as AWS(ELASTIC MAP REDUCE) and AZURE(DATABRICKS).The computation is done by loading the data from an api and actually doing the manipulation in these  services or also loading data into buckets which act as datalakes and then doing the manipulation from there .Familliarise yourself with these cloud services and do these manipulations there ,it can handle big data effectively,fast and at scale.We  end it at there for now.  Hope we have learnt.\n",
    "\n",
    "\n",
    "### Lets learn data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
