# spark_complete_tutorial

In the folder there is the spark.ipynb file it contains and some brief info of how the code works 

In the following repository, we will cover the Spark framework in-depth, from installing it to understanding theoretically how it works, including the transition from RDD to the DataFrame API. We will delve into the differences between them in relation to code implementation and the computational resources required. Additionally, we will explore why developers are increasingly adopting the DataFrame API in Spark. 

Furthermore, we will explore the MLlib library, starting with the basics, how to read DataFrame, handling data quality, performing vector assemblers, building, tuning, and evaluating classification and regression models, and finally, making predictions.

All of this will be included in a single repository. We will also provide theoretical information on handling Spark streaming.

### TABLE OF CONTENTS

1. Installing Spark and Java on our PC.
   
2. Learning about how Spark works and exploring all the new functionalities in Spark 3.0.
 
3. Basics of Spark and how to interact with it.
 
4. PySpark operations on datasets.
  
5. Resilient Distributed Dataset (RDD).
 
6. Spark SQL and how the code differs from using RDDs.
 
7.  MLlib library for handling both classification and regression tasks.
    
8. spark Streaming.


### Lets learn data .hope you learn
